# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LjWd7n2g2vVbexNmXdA4Ub_BoDjz3kia
"""

!wget -O "./Alumni Giving Regression (Edited).csv" "https://www.dropbox.com/s/veak3ugc4wj9luz/Alumni%20Giving%20Regression%20%28Edited%29.csv"

!ls

import pandas as pd
# Replace 'data.csv' with your file name
data = pd.read_csv('Alumni Giving Regression (Edited).csv')

data.head()

from keras.models import Sequential
from keras.layers import Dense, Dropout
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np
from sklearn import linear_model
from sklearn import preprocessing
from sklearn import tree
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import pandas as pd
import csv
import matplotlib.pyplot as plt

np.random.seed(7)
data = pd.read_csv('Alumni Giving Regression (Edited).csv')
dd_df_1 = data.head()

data.head()

data.describe()

corr=data.corr(method ='pearson')

print(corr)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Assuming 'data' is your DataFrame
# Replace this with your actual data loading code
# For example, if your data is in a CSV file, you can use pd.read_csv('your_file.csv')

# Y_POSITION = 5
# model_1_features = [i for i in range(0, Y_POSITION)]
# X = data.iloc[:, model_1_features]
# Y = data.iloc[:, Y_POSITION]

# For demonstration purposes, let's create some random data
data = pd.DataFrame({
    'Feature_1': range(100),
    'Feature_2': range(100, 200),
    'Target': range(50, 150)
})

Y_POSITION = 2  # Assuming 'Target' is the column you want to predict
model_1_features = [i for i in range(0, Y_POSITION)]
X = data.iloc[:, model_1_features]
Y = data.iloc[:, Y_POSITION]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=2020)

# Scatter plot for the training set
plt.scatter(X_train.iloc[:, 0], y_train, label='Training Set')
plt.xlabel('Feature_1')  # Replace with your actual label
plt.ylabel('Target')  # Replace with your actual label
plt.title('Scatter Plot - Training Set')
plt.legend()
plt.show()

# Scatter plot for the testing set
plt.scatter(X_test.iloc[:, 0], y_test, label='Testing Set')
plt.xlabel('Feature_1')  # Replace with your actual label
plt.ylabel('Target')  # Replace with your actual label
plt.title('Scatter Plot - Testing Set')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Assuming 'data' is your DataFrame
# Replace this with your actual data loading code
# For example, if your data is in a CSV file, you can use pd.read_csv('your_file.csv')

# Y_POSITION = 5
# model_1_features = [i for i in range(0, Y_POSITION)]
# X = data.iloc[:, model_1_features]
# Y = data.iloc[:, Y_POSITION]

# For demonstration purposes, let's create some random data
data = pd.DataFrame({
    'Feature_1': range(100),
    'Feature_2': range(100, 200),
    'Target': range(50, 150)
})

Y_POSITION = 2  # Assuming 'Target' is the column you want to predict
model_1_features = [i for i in range(0, Y_POSITION)]
X = data.iloc[:, model_1_features]
Y = data.iloc[:, Y_POSITION]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=2020)

# Scatter plot for the training set
plt.scatter(X_train.iloc[:, 0], y_train, label='Training Set')
plt.xlabel('Feature_1')  # Replace with your actual label
plt.ylabel('Target')  # Replace with your actual label
plt.title('Scatter Plot - Training Set')
plt.legend()
plt.show()

# Scatter plot for the testing set
plt.scatter(X_test.iloc[:, 0], y_test, label='Testing Set')
plt.xlabel('Feature_1')  # Replace with your actual label
plt.ylabel('Target')  # Replace with your actual label
plt.title('Scatter Plot - Testing Set')
plt.legend()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split

# Assuming 'data' is your DataFrame
# Replace this with your actual data loading code
# For example, if your data is in a CSV file, you can use pd.read_csv('your_file.csv')

# For demonstration purposes, let's create some random data
data = pd.DataFrame({
    'Feature_1': range(100),
    'Feature_2': range(100, 200),
    'Feature_3': range(200, 300),
    'Target': range(50, 150)
})

# Set the target column index
Y_POSITION = 2  # Assuming 'Target' is the column you want to predict

# Check the number of columns in your DataFrame
num_columns = len(data.columns)

# Adjust Y_POSITION if it's out of bounds
if Y_POSITION < num_columns:
    # Extract features and target variable
    model_1_features = [i for i in range(0, Y_POSITION)]
    X = data.iloc[:, model_1_features]
    Y = data.iloc[:, Y_POSITION]

    # Now, proceed with train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=2020)
else:
    print(f"Y_POSITION ({Y_POSITION}) is out of bounds for the number of columns ({num_columns}) in the DataFrame.")

Y_POSITION = 4
model_1_features = [i for i in range(0,Y_POSITION)]
X = data.iloc[:,model_1_features]
Y = data.iloc[:,Y_POSITION]
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20,
random_state=2020)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load the dataset (Titanic dataset used as an example)
url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'
data = pd.read_csv(url)

# Select relevant features and target variable
features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']
target = 'Survived'

# Handle missing values (e.g., impute Age with median)
data['Age'].fillna(data['Age'].median(), inplace=True)

# Create X (features) and y (target)
X = data[features]
y = data[target]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling/normalization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and fit a linear regression model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred = model.predict(X_test_scaled)

# Calculate RMSE
RMSE = mean_squared_error(y_test, y_pred, squared=False)
print(f"Linear Regression RMSE: {RMSE}")

# Plotting actual vs predicted values in red (actual) and green (predicted)
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_test, color='red', label='Actual')  # Plotting actual values in red
plt.scatter(y_test, y_pred, color='green', label='Predicted')  # Plotting predicted values in green
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted Values (Linear Regression)')
plt.legend()
plt.show()

